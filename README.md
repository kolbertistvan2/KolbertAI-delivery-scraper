# KolbertAI-delivery-scraper v4.1.0

Returns and shipping info extraction for international webshops in Europe.

## What's new in v4.1.0

### ðŸš€ Major Features
- **Unified JSON Structure**: Pontosan a prompt szerinti JSON struktÃºra minden domainnÃ©l
- **Final Aggregation Script**: `aggregate-final-returns.ts` - deduplikÃ¡ciÃ³ Ã©s sorrend fenntartÃ¡s
- **Country-Specific Data Validation**: Automatikus orszÃ¡g-specifikus adatok ellenÅ‘rzÃ©se
- **Enhanced Data Quality Scoring**: Intelligens adatminÅ‘sÃ©g Ã©rtÃ©kelÃ©s duplikÃ¡tum szÅ±rÃ©shez

### ðŸ”§ Technical Enhancements
- **Consistent JSON Format**: Minden domain ugyanazt a 9 mezÅ‘s struktÃºrÃ¡t kÃ¶veti
- **Robust Deduplication**: Domain normalizÃ¡lÃ¡s Ã©s minÅ‘sÃ©g alapÃº szÅ±rÃ©s
- **Order Preservation**: `websites.txt` sorrend fenntartÃ¡sa a vÃ©gsÅ‘ outputban
- **Error Handling**: Fejlesztett TypeScript error handling Ã©s logging

### ðŸ“Š Data Quality Improvements
- **Structure Validation**: Automatikus JSON struktÃºra tisztÃ­tÃ¡s
- **Duplicate Detection**: Intelligens duplikÃ¡tum felismerÃ©s Ã©s szÅ±rÃ©s
- **Quality Scoring**: AdatminÅ‘sÃ©g alapÃº prioritÃ¡s
- **Country Data Validation**: Helytelen orszÃ¡g-specifikus adatok automatikus felismerÃ©se

### ðŸ“¦ Infrastructure
- **Final Results Directory**: `final-result-returns-v2/` - tisztÃ­tott vÃ©gsÅ‘ eredmÃ©nyek
- **Gitignore Updates**: Results mappÃ¡k automatikus kizÃ¡rÃ¡sa
- **Docker Image Updates**: Automatikus build Ã©s deployment
- **Missing Domain Handling**: HiÃ¡nyzÃ³ domainek placeholder bejegyzÃ©sekkel

## Setup

1. Clone the repository
2. Create `.env` file with your API keys:
   ```
   GOOGLE_API_KEY=your_gemini_api_key
   BROWSERBASE_API_KEY=your_browserbase_api_key
   BROWSERBASE_PROJECT_ID=your_project_id
   ```
3. Build Docker image: `docker build -t kolbertai-shipping-extractor .`
4. Run: `./run-batches.sh`

## Usage

### Basic Usage
- Add domains to `websites.txt`
- Set `BATCH_SIZE` environment variable for parallel processing
- Results saved to `result-returns-v2/` directory

### Final Aggregation
After running the scraper, use the final aggregation script:
```bash
npx tsx aggregate-final-returns.ts
```

This will:
- Read all JSON files from `result-returns-v2/`
- Deduplicate domains based on quality scoring
- Maintain exact order from `websites.txt`
- Clean JSON structure to match prompt format exactly
- Output final results to `final-result-returns-v2/`

### Failed Domains Processing
If some domains fail during processing, you can retry them:

1. **Create failed domains list:**
```bash
# Extract failed domains from log files
ls result-returns-v2/failed-*.log | sed 's/result-returns-v2\/failed-//' | sed 's/-[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}T[0-9]\{2\}-[0-9]\{2\}-[0-9]\{2\}-[0-9]\{3\}Z\.log$//' | sort > failed-domains.txt
```

2. **Process failed domains:**
```bash
# Process failed domains individually
npx tsx extract-returns-v2.ts failed-domains.txt

# Or use Docker batch processing
docker run -v $(pwd):/app -e BATCH_SIZE=5 kolbertai-shipping-extractor failed-domains.txt
```

3. **Re-run aggregation:**
```bash
# Re-aggregate with new results
npx tsx aggregate-final-returns.ts
```

### Error Recovery Workflow
1. **Check failed domains:** Look for `failed-*.log` files in `result-returns-v2/`
2. **Create retry list:** Extract domain names from failed logs
3. **Retry processing:** Run failed domains again
4. **Re-aggregate:** Update final results with new data
5. **Verify results:** Check final output in `final-result-returns-v2/`

### Output Structure
The final JSON follows exactly the prompt structure:
```json
{
  "domain.com": {
    "country": "not available",
    "IN_STORE_RETURN": { ... },
    "HOME_COLLECTION": { ... },
    "DROP_OFF_PARCEL_SHOP": { ... },
    "DROP_OFF_PARCEL_LOCKER": { ... },
    "FREE_RETURN": { ... },
    "QR_CODE_BARCODE_PIN": { ... },
    "EXTERNAL_RETURN_PORTAL": { ... },
    "SUMMARY": "..."
  }
}
```

## File Structure

```
â”œâ”€â”€ extract-returns-v2.ts          # Main scraper
â”œâ”€â”€ aggregate-final-returns.ts     # Final aggregation script
â”œâ”€â”€ prompt-return.txt              # Return extraction prompt
â”œâ”€â”€ websites.txt                   # Domain list
â”œâ”€â”€ failed-domains.txt             # Failed domains for retry (optional)
â”œâ”€â”€ result-returns-v2/             # Raw results (gitignored)
â”œâ”€â”€ final-result-returns-v2/       # Final results (gitignored)
â””â”€â”€ .gitignore                     # Excludes results folders
```

## Data Quality Features

- **Automatic Structure Cleaning**: Removes extra fields not in prompt
- **Country Data Validation**: Detects incorrect country-specific data
- **Quality Scoring**: Prioritizes better quality data during deduplication
- **Order Preservation**: Maintains exact order from `websites.txt`
- **Duplicate Handling**: Intelligently removes duplicates based on quality
