# KolbertAI-delivery-scraper v4.1.0

Returns and shipping info extraction for international webshops in Europe.

## What's new in v4.1.0

### üöÄ Major Features
- **Unified JSON Structure**: Pontosan a prompt szerinti JSON strukt√∫ra minden domainn√©l
- **Final Aggregation Script**: `aggregate-final-returns.ts` - deduplik√°ci√≥ √©s sorrend fenntart√°s
- **Country-Specific Data Validation**: Automatikus orsz√°g-specifikus adatok ellen≈ërz√©se
- **Enhanced Data Quality Scoring**: Intelligens adatmin≈ës√©g √©rt√©kel√©s duplik√°tum sz≈±r√©shez

### üîß Technical Enhancements
- **Consistent JSON Format**: Minden domain ugyanazt a 9 mez≈ës strukt√∫r√°t k√∂veti
- **Robust Deduplication**: Domain normaliz√°l√°s √©s min≈ës√©g alap√∫ sz≈±r√©s
- **Order Preservation**: `websites.txt` sorrend fenntart√°sa a v√©gs≈ë outputban
- **Error Handling**: Fejlesztett TypeScript error handling √©s logging

### üìä Data Quality Improvements
- **Structure Validation**: Automatikus JSON strukt√∫ra tiszt√≠t√°s
- **Duplicate Detection**: Intelligens duplik√°tum felismer√©s √©s sz≈±r√©s
- **Quality Scoring**: Adatmin≈ës√©g alap√∫ priorit√°s
- **Country Data Validation**: Helytelen orsz√°g-specifikus adatok automatikus felismer√©se

### üì¶ Infrastructure
- **Final Results Directory**: `final-result-returns-v2/` - tiszt√≠tott v√©gs≈ë eredm√©nyek
- **Gitignore Updates**: Results mapp√°k automatikus kiz√°r√°sa
- **Docker Image Updates**: Automatikus build √©s deployment
- **Missing Domain Handling**: Hi√°nyz√≥ domainek placeholder bejegyz√©sekkel

## Setup

1. Clone the repository
2. Create `.env` file with your API keys:
   ```
   GOOGLE_API_KEY=your_gemini_api_key
   BROWSERBASE_API_KEY=your_browserbase_api_key
   BROWSERBASE_PROJECT_ID=your_project_id
   ```
3. Build Docker image: `docker build -t kolbertai-shipping-extractor .`
4. Run: `./run-batches.sh`

## Usage

### Basic Usage
- Add domains to `websites.txt`
- Set `BATCH_SIZE` environment variable for parallel processing
- Results saved to `result-returns-v2/` directory

### Final Aggregation
After running the scraper, use the final aggregation script:
```bash
npx tsx aggregate-final-returns.ts
```

This will:
- Read all JSON files from `result-returns-v2/`
- Deduplicate domains based on quality scoring
- Maintain exact order from `websites.txt`
- Clean JSON structure to match prompt format exactly
- Output final results to `final-result-returns-v2/`

### Output Structure
The final JSON follows exactly the prompt structure:
```json
{
  "domain.com": {
    "country": "not available",
    "IN_STORE_RETURN": { ... },
    "HOME_COLLECTION": { ... },
    "DROP_OFF_PARCEL_SHOP": { ... },
    "DROP_OFF_PARCEL_LOCKER": { ... },
    "FREE_RETURN": { ... },
    "QR_CODE_BARCODE_PIN": { ... },
    "EXTERNAL_RETURN_PORTAL": { ... },
    "SUMMARY": "..."
  }
}
```

## File Structure

```
‚îú‚îÄ‚îÄ extract-returns-v2.ts          # Main scraper
‚îú‚îÄ‚îÄ aggregate-final-returns.ts     # Final aggregation script
‚îú‚îÄ‚îÄ prompt-return.txt              # Return extraction prompt
‚îú‚îÄ‚îÄ websites.txt                   # Domain list
‚îú‚îÄ‚îÄ result-returns-v2/             # Raw results (gitignored)
‚îú‚îÄ‚îÄ final-result-returns-v2/       # Final results (gitignored)
‚îî‚îÄ‚îÄ .gitignore                     # Excludes results folders
```

## Data Quality Features

- **Automatic Structure Cleaning**: Removes extra fields not in prompt
- **Country Data Validation**: Detects incorrect country-specific data
- **Quality Scoring**: Prioritizes better quality data during deduplication
- **Order Preservation**: Maintains exact order from `websites.txt`
- **Duplicate Handling**: Intelligently removes duplicates based on quality
